{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BPR\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../gamedata/original_dataset/\"\n",
    "DATA_FILE = \"dd_event_schemas.csv\"\n",
    "DATA_FOR_BPR = \"data_for_BPR.csv\"\n",
    "SAVE_DIR = \"../gamedata/train_result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #data preprocessing\n",
    "# data = pd.read_csv(DATA_DIR+DATA_FILE)\n",
    "# gameid = data['gameid'].drop_duplicates()\n",
    "# gameid_list = []\n",
    "# gameid_list = list(gameid)\n",
    "# gameid2newid = {old:new+1 for new, old in enumerate(gameid_list)}\n",
    "# data['new_game_id'] = data['gameid'].map(gameid2newid)\n",
    "\n",
    "# events = data['eventname'].drop_duplicates()\n",
    "# events_list = list(events)\n",
    "# events2id = {old:new+1 for new,old in enumerate(events_list)}\n",
    "# data['new_event_id'] = data['eventname'].map(events2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv(DATA_DIR+'data_for_BPR.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data=pd.read_csv(DATA_DIR+'data_for_BPR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data[data['new_game_id']==63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_count: 1108\n",
      "event_count: 11189\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    game_events = defaultdict(set)\n",
    "    game_count = -1\n",
    "    event_count = -1\n",
    "    with open(data_path,'r') as f:\n",
    "        header_line = next(f)\n",
    "        for line in f.readlines():\n",
    "            _, _, _, _, _, _, gameid, eventid = line.split(\",\")\n",
    "            gameid = int(gameid)\n",
    "            eventid = int(eventid)\n",
    "            #print(u,i)\n",
    "            game_events[gameid].add(eventid)\n",
    "            game_count = max(gameid, game_count)\n",
    "            event_count = max(eventid, event_count)\n",
    "    print (\"game_count:\", game_count)\n",
    "    print (\"event_count:\", event_count)\n",
    "    #return max_u_id, max_i_id, user_ratings\n",
    "    return game_count,event_count,game_events\n",
    "    \n",
    "\n",
    "#data_path = os.path.join('D:\\\\tmp\\\\ml-100k', 'u.data')\n",
    "#user_count, item_count, user_ratings = load_data(data_path)\n",
    "game_count,event_count,game_events_dict = load_data(DATA_DIR+DATA_FOR_BPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "def generate_test(game_events_dict):\n",
    "    user_test = dict()\n",
    "    for u, i_list in game_events_dict.items():\n",
    "        user_test[u] = random.sample(game_events_dict[u], 1)[0]\n",
    "    return user_test\n",
    "\n",
    "game_test_set = generate_test(game_events_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate dataset for training\n",
    "def generate_train_batch(game_events_dict, game_test_set, event_count, batch_size=128):\n",
    "    t = []\n",
    "    \n",
    "    #print('generate batch..')\n",
    "    for b in range(batch_size):\n",
    "        break_flag1 = 0\n",
    "        break_flag2 = 0\n",
    "        u = random.sample(game_events_dict.keys(), 1)[0]\n",
    "        #print('u:',u)\n",
    "        i = random.sample(game_events_dict[u], 1)[0]\n",
    "        while i == game_test_set[u]:\n",
    "            break_flag1 += 1\n",
    "            i = random.sample(game_events_dict[u], 1)[0]\n",
    "            if break_flag1 == 20:\n",
    "                #game_events_dict.pop('u',None)\n",
    "                #game_test_set.pop('u',None)\n",
    "                break\n",
    "            #print('i:',i)\n",
    "        j = random.randint(1, event_count)\n",
    "        while j in game_events_dict[u]:\n",
    "            break_flag2 += 1\n",
    "            j = random.randint(1, event_count)\n",
    "            if break_flag2 == 20:\n",
    "                #game_events_dict.pop('u',None)\n",
    "                #game_test_set.pop('u',None)\n",
    "                break\n",
    "            #print('j:',j)\n",
    "        t.append([u, i, j])\n",
    "        #print(u,i,j)\n",
    "    #print('generate batch done!')\n",
    "    return numpy.asarray(t)\n",
    "\n",
    "#train_set = generate_train_batch(game_events_dict,game_test_set,event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_batch(game_events_dict, game_test_set, event_count):\n",
    "    for u in game_events_dict.keys():\n",
    "        t = []\n",
    "        i = game_test_set[u]\n",
    "        for j in range(1, event_count+1):\n",
    "            if not (j in game_events_dict[u]):\n",
    "                t.append([u, i, j])\n",
    "        yield numpy.asarray(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bpr_mf(game_count, event_count, hidden_dim):\n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        user_emb_w = tf.get_variable(\"user_emb_w\", [game_count+1, hidden_dim], \n",
    "                            initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        item_emb_w = tf.get_variable(\"item_emb_w\", [event_count+1, hidden_dim], \n",
    "                                initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        \n",
    "        u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n",
    "        i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n",
    "        j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n",
    "    \n",
    "    # MF predict: u_i > u_j\n",
    "    x = tf.reduce_sum(tf.multiply(u_emb, (i_emb - j_emb)), 1, keepdims=True)\n",
    "    \n",
    "    # AUC for one user:\n",
    "    # reasonable iff all (u,i,j) pairs are from the same user\n",
    "    # \n",
    "    # average AUC = mean( auc for each user in test set)\n",
    "    mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
    "    \n",
    "    l2_norm = tf.add_n([\n",
    "            tf.reduce_sum(tf.multiply(u_emb, u_emb)), \n",
    "            tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n",
    "            tf.reduce_sum(tf.multiply(j_emb, j_emb))\n",
    "        ])\n",
    "    \n",
    "    regulation_rate = 0.0001\n",
    "    bprloss = regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n",
    "    \n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(bprloss)\n",
    "    return u, i, j, mf_auc, bprloss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"AddN:0\", shape=(), dtype=float32) Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "start epoch\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4999/4999 [00:24<00:00, 205.17it/s]\n",
      "6it [00:00, 33.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpr_loss:  0.7004937100443847\n",
      "_train_op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1108it [00:18, 60.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:  1.3384761717775668 test_auc:  0.5112981665390922\n",
      "\n",
      "Variable:  user_emb_w:0\n",
      "Shape:  (1109, 20)\n",
      "[[-0.07827675  0.03325282  0.03547279 ...  0.24047272  0.08952945\n",
      "  -0.09412265]\n",
      " [-0.12227901  0.008841    0.14813305 ...  0.1654895   0.08843858\n",
      "  -0.04588021]\n",
      " [-0.06300459  0.10307686  0.07544495 ... -0.12270406  0.0579744\n",
      "  -0.07400693]\n",
      " ...\n",
      " [-0.09181725 -0.02721917  0.01927665 ... -0.03661722 -0.0572461\n",
      "  -0.00751277]\n",
      " [-0.09103559  0.11260398 -0.08012318 ...  0.07623894 -0.04960154\n",
      "  -0.02741719]\n",
      " [ 0.05854981 -0.00831366  0.16878232 ...  0.05671526  0.10351533\n",
      "   0.02996822]]\n",
      "Variable:  item_emb_w:0\n",
      "Shape:  (11190, 20)\n",
      "[[ 1.5270211e-02 -3.5029404e-02 -2.8490314e-02 ... -7.3622190e-03\n",
      "  -1.4807060e-02 -1.5188259e-01]\n",
      " [ 5.5210702e-02  1.1759946e-01 -1.6139945e-02 ...  3.2923778e-03\n",
      "   1.3766246e-04  5.4466449e-02]\n",
      " [-1.4415974e-02  7.5199150e-02 -6.2056758e-02 ...  1.6244659e-02\n",
      "  -3.0633692e-02  1.0007460e-02]\n",
      " ...\n",
      " [ 6.3158825e-02 -3.8425114e-02 -1.6166653e-02 ...  2.4558902e-03\n",
      "  -7.6136202e-02  1.4824350e-01]\n",
      " [ 3.1846978e-02 -6.7941241e-02  4.6800994e-04 ...  2.2416860e-02\n",
      "   6.5406403e-03  1.2422439e-01]\n",
      " [-1.1710351e-01 -5.9977390e-02  3.6644112e-02 ... -2.0518415e-02\n",
      "  -6.1136544e-02  1.7391993e-01]]\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    u, i, j, mf_auc, bprloss, train_op = bpr_mf(game_count, event_count, 20)\n",
    "#     print(u, i, j, mf_auc, bprloss, train_op)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    print('start epoch')\n",
    "    for epoch in range(1, 2):\n",
    "        print (\"epoch: \", epoch)\n",
    "        _batch_bprloss = 0\n",
    "        for k in tqdm(range(1, 5000)): # uniform samples from training set\n",
    "            uij = generate_train_batch(game_events_dict, game_test_set,event_count)\n",
    "\n",
    "            _bprloss, _train_op = session.run([bprloss, train_op], \n",
    "                                feed_dict={u:uij[:,0], i:uij[:,1], j:uij[:,2]})\n",
    "            _batch_bprloss += _bprloss\n",
    "        \n",
    "        \n",
    "        print (\"bpr_loss: \", _batch_bprloss / k)\n",
    "        print (\"_train_op\")\n",
    "\n",
    "        games_count = 0\n",
    "        _auc_sum = 0.0\n",
    "        test_bprloss = 0.0\n",
    "        # each batch will return only one user's auc\n",
    "        for t_uij in tqdm(generate_test_batch(game_events_dict, game_test_set, event_count)):\n",
    "\n",
    "            _auc, _test_bprloss = session.run([mf_auc, bprloss],\n",
    "                                    feed_dict={u:t_uij[:,0], i:t_uij[:,1], j:t_uij[:,2]}\n",
    "                                )\n",
    "            games_count += 1\n",
    "            _auc_sum += _auc\n",
    "            test_bprloss += _test_bprloss\n",
    "#            print (\"test_loss: \", test_bprloss/games_count)\n",
    "        print (\"test_loss: \", test_bprloss/games_count, \"test_auc: \", _auc_sum/games_count)\n",
    "        print (\"\")\n",
    "    variable_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = session.run(variable_names)\n",
    "    saver.save(session, SAVE_DIR)\n",
    "    for k,v in zip(variable_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "        print(v)\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2801418"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_bprloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.save(session, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = tf.Session()\n",
    "u1_dim = tf.expand_dims(values[0][0], 0)\n",
    "u1_all = tf.matmul(u1_dim, values[1],transpose_b=True)\n",
    "result_1 = session1.run(u1_all)\n",
    "print (result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"new recommedation ：\")\n",
    "p = numpy.squeeze(result_1)\n",
    "p[numpy.argsort(p)[:-5]] = 0\n",
    "for index in range(len(p)):\n",
    "    if p[index] != 0:\n",
    "        select = data[data['new_event_id']==index].drop_duplicates('eventname')\n",
    "  \n",
    "        print (index, p[index],select['eventname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(DATA_DIR+'data_for_BPR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['new_event_id']==11118]\n",
    "data[data['new_game_id']==1].drop_duplicates('eventname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
